{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as ran\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ContinuousControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityPolicy(ContinuousControlNet.ContinuousControlNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(.1, 5)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return obs\n",
    "    \n",
    "    def getUnclonedCopy(self):\n",
    "        return type(self)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1121e-07)\n",
      "tensor(2.1121e-07)\n",
      "2.1121052370629137e-07\n"
     ]
    }
   ],
   "source": [
    "#Test to assert that we compute the normal probability correctly\n",
    "#And that it doesn't change between the returned value and calling action prob\n",
    "policy = IdentityPolicy()\n",
    "ran.seed(2)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "obs = ran.normal(0, 1, (5))\n",
    "action, prob = policy.act(obs)\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.linalg\n",
    "\n",
    "#Check these are all the same\n",
    "print(prob)\n",
    "print(policy.computeActionProbFromObs(obs, action))\n",
    "print(multivariate_normal.pdf(action, mean = obs, cov = 0.1**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41675785 -0.05626683 -2.1361961   1.64027081 -1.79343559]\n",
      "(array([-0.41675785, -0.05626683, -2.1361961 ,  1.6402708 , -1.7934356 ],\n",
      "      dtype=float32), 1)\n",
      "(array([-0.41675785, -0.05626683, -2.1361961 ,  1.6402708 , -1.7934356 ],\n",
      "      dtype=float32), 1)\n",
      "(array([-0.68294174,  0.10275824, -2.52999081,  1.30571696, -2.08088902]), tensor(2.1121e-07))\n"
     ]
    }
   ],
   "source": [
    "#Test to assert that eval() actions are deterministic and that eval actions after cloning are the same\n",
    "#But that these are not the same as training vals\n",
    "ran.seed(2)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "policy = IdentityPolicy()\n",
    "obs = ran.normal(0, 1, (5))\n",
    "\n",
    "policy.eval()\n",
    "print(obs)\n",
    "print(policy.act(obs))\n",
    "print(policy.act(obs))\n",
    "policy.train()\n",
    "print(policy.act(obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy(ContinuousControlNet.ContinuousControlNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(.1, 1)\n",
    "        self.fc = nn.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        return self.fc(obs)\n",
    "    \n",
    "    def getUnclonedCopy(self):\n",
    "        return type(self)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.42809087], dtype=float32), 1)\n",
      "(array([-0.42809087], dtype=float32), 1)\n",
      "(array([1.8498709], dtype=float32), 1)\n"
     ]
    }
   ],
   "source": [
    "#Test to assert that eval() actions are deterministic across clones\n",
    "ran.seed(2)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "policy = SimplePolicy()\n",
    "policy.eval()\n",
    "clonePolicy = policy.clone()\n",
    "clonePolicy.eval()\n",
    "\n",
    "print(policy.act(obs))\n",
    "print(clonePolicy.act(obs)) #Should be the same due to parameter cloning\n",
    "\n",
    "newPolicy = SimplePolicy()\n",
    "newPolicy.eval()\n",
    "print(newPolicy.act(obs)) #Should be different due to different random parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before first gradient\n",
      "(array([-0.42809087], dtype=float32), 1)\n",
      "(array([-0.42809087], dtype=float32), 1)\n",
      "After first gradient)\n",
      "(array([-0.35766163], dtype=float32), 1)\n",
      "(array([-0.42809087], dtype=float32), 1)\n",
      "After second gradient\n",
      "(array([-0.40134367], dtype=float32), 1)\n",
      "(array([-0.42809087], dtype=float32), 1)\n"
     ]
    }
   ],
   "source": [
    "#Confirm that gradient descent only effects a policy and not its clone\n",
    "ran.seed(2)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "policy = SimplePolicy() #the policy we'll be training\n",
    "newPolicy = policy.clone() #control \n",
    "newPolicy.eval()\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 0.01)\n",
    "\n",
    "obs = ran.normal(0, 1, (5))\n",
    "\n",
    "policy.eval()\n",
    "print('Before first gradient')\n",
    "print(policy.act(obs))\n",
    "print(newPolicy.act(obs))\n",
    "\n",
    "policy.train()\n",
    "optimizer.zero_grad()\n",
    "#Find something arbitrary to do gradient descent on. In this case, the action prob itself\n",
    "action, _ = policy.act(obs)\n",
    "action_prob = policy.computeActionProbFromObs(obs, action)\n",
    "#print(action_prob)\n",
    "torch.autograd.backward(action_prob)\n",
    "#[print(parameter.grad) for parameter in [parameters for parameters in policy.parameters()]]\n",
    "optimizer.step()\n",
    "\n",
    "policy.eval()\n",
    "#See if the old policy or the new policy have changed after the optimizer\n",
    "print('After first gradient)')\n",
    "print(policy.act(obs))\n",
    "print(newPolicy.act(obs))\n",
    "\n",
    "policy.train()\n",
    "#repeat once more\n",
    "optimizer.zero_grad()\n",
    "action,_ = policy.act(obs)\n",
    "action_prob = policy.computeActionProbFromObs(obs, action)\n",
    "torch.autograd.backward(action_prob)\n",
    "optimizer.step()\n",
    "\n",
    "policy.eval()\n",
    "print('After second gradient')\n",
    "print(policy.act(obs))\n",
    "print(newPolicy.act(obs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
